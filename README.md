# ChatCal.ai

An AI-powered Google Calendar booking assistant with multiple LLM integrations, local model support, and **persistent OAuth authentication** powered by Google Cloud Secret Manager.

## Overview

ChatCal.ai is a comprehensive calendar booking solution that combines the power of large language models with Google Calendar integration. The project includes both cloud-based LLM options (Anthropic Claude, Google Gemini) and local model support through Hugging Face transformers.

### üöÄ Latest: Secret Manager OAuth Persistence (v0.3.4)
**Problem Solved**: OAuth tokens previously expired after 15 minutes when Cloud Run containers restarted, forcing users to re-authenticate frequently.

**Solution**: Integrated Google Cloud Secret Manager for persistent OAuth token storage across container restarts. Now users maintain authenticated sessions even after container lifecycle events.

**Technical Achievement**: Multi-tier storage system with Secret Manager (persistent) ‚Üí In-memory cache (fast) ‚Üí Redis/File fallbacks.

## Project Structure

```
chatcal.ai/
‚îú‚îÄ‚îÄ chatcal-ai/           # Main application
‚îÇ   ‚îú‚îÄ‚îÄ app/              # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ frontend/         # Web UI
‚îÇ   ‚îú‚îÄ‚îÄ docker/           # Containerization
‚îÇ   ‚îî‚îÄ‚îÄ docs/             # Documentation
‚îú‚îÄ‚îÄ hf-lama.py           # Local LLM with Hugging Face & LlamaIndex
‚îú‚îÄ‚îÄ llama-anthropic.py   # Anthropic Claude integration
‚îú‚îÄ‚îÄ models/              # Downloaded Hugging Face models
‚îî‚îÄ‚îÄ docs/                # Project documentation
```

## Components

### 1. ChatCal.ai Core Application
- **Location**: `./chatcal-ai/`
- **Description**: Full-featured calendar booking assistant with web interface
- **Features**: 
  - Google Calendar integration
  - FastAPI backend with Redis sessions
  - Docker containerization
  - OAuth2 authentication
  - Multiple LLM provider support

### 2. Local LLM Integration (hf-lama.py)
- **Purpose**: Run language models locally using Hugging Face transformers
- **Features**:
  - Support for Qwen2.5, Phi-3, and other open models
  - LlamaIndex integration for RAG capabilities
  - Document loading and vector indexing
  - Interactive chat interface
  - SSL certificate handling for NLTK downloads

### 3. Cloud LLM Scripts
- **llama-anthropic.py**: Integration with Anthropic's Claude API
- **simpletest.py**: Testing utilities

## Quick Start

### Option 1: Full Application (Recommended)
```bash
cd chatcal-ai
docker-compose up -d
```
Access at: http://localhost:8000

### Option 2: Local LLM Only
```bash
python3 hf-lama.py
```

## Requirements

### For Full Application:
- Docker & Docker Compose
- Google Cloud Project with Calendar API
- Anthropic API key (optional)
- Google Gemini API key (optional)

### For Local LLM:
- Python 3.11+
- PyTorch
- Transformers
- LlamaIndex
- ~8GB+ RAM (for 7B models)

## Configuration

1. **Google Calendar Setup**: Follow the guide in `chatcal-ai/docs/google-oauth-setup.md`
2. **API Keys**: Set environment variables for your chosen LLM providers
3. **Model Selection**: Edit `hf-lama.py` to choose your preferred local model

## Available Models (Local)

- **Qwen2.5-7B-Instruct**: High-quality general purpose model
- **Qwen2.5-3B-Instruct**: Lighter version for resource-constrained environments  
- **Qwen2.5-1.5B-Instruct**: Ultra-lightweight option
- **Phi-3-mini-128k**: Microsoft's efficient model with large context

## Features

### Cloud Integration
- ‚úÖ Anthropic Claude (Sonnet, Haiku)
- ‚úÖ Google Gemini Pro
- ‚úÖ Google Calendar API
- ‚úÖ OAuth2 authentication with **Secret Manager persistence**
- ‚úÖ Session management
- üöÄ **NEW**: Persistent OAuth across container restarts (v0.3.4)

### Local Capabilities
- ‚úÖ Local model inference
- ‚úÖ Document RAG with LlamaIndex
- ‚úÖ Vector embeddings
- ‚úÖ Interactive chat
- ‚úÖ SSL/NLTK handling

### Deployment
- ‚úÖ Docker containerization
- ‚úÖ Docker Compose orchestration  
- ‚úÖ Development & production configs
- ‚úÖ Health checks & monitoring
- üîê **Google Cloud Secret Manager integration**
- ‚è±Ô∏è **Persistent OAuth tokens across container restarts**

## Documentation

- [Main Application README](./chatcal-ai/README.md) - Detailed setup instructions
- [Google OAuth Setup](./chatcal-ai/docs/google-oauth-setup.md) - Calendar integration guide
- [LinkedIn Integration](./chatcal-ai/docs/linkedin-google-calendar-integration.md) - Business use cases
- [Deployment Guide](./chatcal-ai/DEPLOYMENT.md) - Production deployment

## Development

### Testing Local Models
```bash
# Test different model sizes
python3 hf-lama.py  # Uses Qwen2.5-7B by default

# Edit the script to try other models:
# config = model_configs["qwen2.5-3b"]  # Lighter option
# config = model_configs["phi3-mini"]   # Microsoft model
```

### Contributing
1. Fork the repository
2. Create a feature branch
3. Test your changes with both local and cloud models
4. Submit a pull request

## Support

- **Issues**: Open GitHub issues for bugs or feature requests
- **Documentation**: Check the `docs/` directories for detailed guides
- **Contact**: pgits.job@gmail.com

## License

See individual component licenses in their respective directories.